{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f11d5f-7dbb-41bb-8020-3af75e2edd12",
   "metadata": {},
   "source": [
    "1.Purpose:\n",
    "\n",
    "Hyperparameter Tuning: Grid Search CV (Cross-Validation) is used to find the optimal hyperparameters for a machine learning model. Hyperparameters are settings that cannot be learned from the data directly but significantly affect the performance of the model.\n",
    "How It Works:\n",
    "\n",
    "Define Hyperparameter Space: Specify the range of hyperparameters to search over. For example, the values of C and gamma for an SVM.\n",
    "Cross-Validation: For each combination of hyperparameters, perform k-fold cross-validation.\n",
    "Evaluate Performance: Calculate the performance metric (e.g., accuracy, F1 score) for each combination.\n",
    "Select Best Hyperparameters: Choose the combination with the best cross-validation performance.\n",
    "Refit Model: Refit the model using the entire training dataset with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568890-d451-4b8b-b45f-43efb7b8bda0",
   "metadata": {},
   "source": [
    "2.Grid Search CV:\n",
    "\n",
    "Exhaustive Search: Evaluates all possible combinations of the specified hyperparameter grid.\n",
    "Pros: Comprehensive and guarantees finding the best combination within the provided grid.\n",
    "Cons: Computationally expensive and time-consuming, especially with a large number of hyperparameters and ranges.\n",
    "Randomized Search CV:\n",
    "\n",
    "Random Sampling: Evaluates a fixed number of random combinations from the specified hyperparameter space.\n",
    "Pros: More efficient and faster, especially with a large hyperparameter space. Reduces computation time by not evaluating every combination.\n",
    "Cons: Does not guarantee finding the absolute best combination but often finds a good one.\n",
    "When to Choose:\n",
    "\n",
    "Grid Search CV: When the hyperparameter space is small and you can afford the computational cost.\n",
    "Randomized Search CV: When the hyperparameter space is large, and you need a more efficient search strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c5050-b08c-44d6-8099-7d1b297679e8",
   "metadata": {},
   "source": [
    "3.Data Leakage:\n",
    "\n",
    "Definition: Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates.\n",
    "Why It's a Problem:\n",
    "\n",
    "Overfitting: The model learns from information it shouldn't have access to, leading to high performance on the training data but poor generalization to new, unseen data.\n",
    "Example:\n",
    "\n",
    "Scenario: Predicting future stock prices using features that include future information (e.g., future closing prices). If these future prices are included in the training set, the model will perform unrealistically well but fail in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b258e97-de58-4454-b462-f0b5f74d0bd1",
   "metadata": {},
   "source": [
    "4.Prevention Strategies:\n",
    "\n",
    "Proper Data Splitting: Ensure that the test set is truly representative of future, unseen data. Split the data chronologically if dealing with time series data.\n",
    "Feature Engineering: Perform feature engineering on the training set separately before splitting or using cross-validation.\n",
    "Cross-Validation: Use proper cross-validation techniques that maintain the integrity of the training and validation sets.\n",
    "Pipeline Usage: Use pipelines to ensure that any preprocessing steps are applied separately to training and test data.\n",
    "Exclude Leaky Features: Identify and exclude features that contain information that would not be available at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09456610-b3a7-4015-a81e-9e991ab62f3f",
   "metadata": {},
   "source": [
    "5.Confusion Matrix:\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing the actual and predicted classes.\n",
    "Structure:\n",
    "\n",
    "True Positive (TP): Correctly predicted positive instances.\n",
    "True Negative (TN): Correctly predicted negative instances.\n",
    "False Positive (FP): Incorrectly predicted positive instances (Type I error).\n",
    "False Negative (FN): Incorrectly predicted negative instances (Type II error).\n",
    "Insights:\n",
    "\n",
    "Shows the model's ability to correctly classify instances and the types of errors it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08792944-a2ef-4396-a0b6-25cdd6d9852b",
   "metadata": {},
   "source": [
    "6.Precision:\n",
    "\n",
    "Definition: The proportion of true positive predictions among all positive predictions.\n",
    "Formula:\n",
    "Precision\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "Focus: Measures the accuracy of positive predictions.\n",
    "Recall:\n",
    "\n",
    "Definition: The proportion of true positive predictions among all actual positive instances.\n",
    "Formula:\n",
    "Recall\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "Focus: Measures the ability to identify all positive instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e1bf8-3c22-4f8d-8ff9-59b1dc6f6638",
   "metadata": {},
   "source": [
    "7.Error Types:\n",
    "\n",
    "False Positives (FP): Instances where the model incorrectly predicts the positive class.\n",
    "False Negatives (FN): Instances where the model incorrectly predicts the negative class.\n",
    "Interpretation:\n",
    "\n",
    "High FP: Indicates the model is often predicting positives incorrectly. This may be critical in scenarios like medical diagnoses where false alarms should be minimized.\n",
    "High FN: Indicates the model is missing positive cases. This is crucial in scenarios like fraud detection where missing fraudulent cases is highly undesirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2335fe6-e4f7-4fb3-8d90-aea639dbf5c5",
   "metadata": {},
   "source": [
    "8.Common Metrics:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝑇\n",
    "𝑁\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝑇\n",
    "𝑁\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "Accuracy= \n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n",
    " \n",
    "Precision:\n",
    "\n",
    "Precision\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Recall\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "Specificity (True Negative Rate):\n",
    "\n",
    "Specificity\n",
    "=\n",
    "𝑇\n",
    "𝑁\n",
    "𝑇\n",
    "𝑁\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "Specificity= \n",
    "TN+FP\n",
    "TN\n",
    "​\n",
    " \n",
    "F1 Score:\n",
    "\n",
    "F1 Score\n",
    "=\n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1 Score=2× \n",
    "Precision+Recall\n",
    "Precision×Recall\n",
    "​\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2857a-dcc6-40a3-9965-f62682f679f2",
   "metadata": {},
   "source": [
    "9.Accuracy:\n",
    "\n",
    "Definition: The proportion of correctly predicted instances (both true positives and true negatives) among the total instances.\n",
    "Formula:\n",
    "Accuracy\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝑇\n",
    "𝑁\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝑇\n",
    "𝑁\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "Accuracy= \n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n",
    " \n",
    "Relationship:\n",
    "\n",
    "Dependence: Accuracy depends on the values of TP, TN, FP, and FN in the confusion matrix.\n",
    "Imbalance Sensitivity: Accuracy can be misleading in imbalanced datasets because it does not account for the distribution of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb16018-2968-46cb-89c3-663d157f0c0b",
   "metadata": {},
   "source": [
    "10.Identifying Biases:\n",
    "\n",
    "Class Imbalance: A large disparity between FN and FP can indicate that the model is biased towards the majority class.\n",
    "Error Patterns: Consistent misclassification of certain classes can suggest model bias or limitations in feature representation.\n",
    "Threshold Analysis: Adjusting the classification threshold and analyzing the resulting confusion matrix can reveal bias towards precision or recall.\n",
    "Actions:\n",
    "\n",
    "Resampling Techniques: Address class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "Feature Engineering: Improve features that help distinguish between classes better.\n",
    "Regularization: Apply techniques to reduce overfitting and bias towards certain classes.\n",
    "Model Choice: Consider using different models or ensemble methods that can handle class imbalance better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487511a0-0b27-4f41-947e-e9d28cf89615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
